{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import helper as hp\n",
    "import os\n",
    "import pandas as pd\n",
    "#import matplolib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Path to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To begin the translation is made only with a small text from english to spanish, since the training might take days even weeks using a real dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_path = 'small_vocab_en'\n",
    "target_path = 'small_vocab_fr'\n",
    "#Padding since not all senteces will have the same size, EOS is the end of sentence\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3}\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_load(path):\n",
    "    files = os.path.join(path)\n",
    "    files = files.encode('utf-8')\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = hp.load_file(input_file)\n",
    "\n",
    "## Loading the two small texts\n",
    "input_file = hp.load_file(input_file)\n",
    "target_file = hp.load_file(target_file)\n",
    "#spanish_file = spanish_file.encode('utf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9847064\n"
     ]
    }
   ],
   "source": [
    "x = len(target_file)\n",
    "print(x)\n",
    "x = int(x/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
      "la chaux est son moins aimé des fruits , mais la banane est mon moins aimé.\n",
      "il a vu un vieux camion jaune .\n",
      "inde est pluvieux en juin , et il est parfois chaud en novembre .\n",
      "ce chat était mon animal le plus aimé .\n",
      "il n'aime\n"
     ]
    }
   ],
   "source": [
    "print(target_file[:1000])\n",
    "#eng_file = hp.load_file(input_file[:x])\n",
    "#spanish_file = hp.load_file(target_file[:x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "the lime is her least liked fruit , but the banana is my least liked .\n",
      "he saw a old yellow truck .\n",
      "india is rainy during june , and it is sometimes warm in november .\n",
      "that cat was my most loved animal .\n",
      "he dislikes grapefruit , limes , and lem\n"
     ]
    }
   ],
   "source": [
    "print(input_file[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Counting number of unique words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_words_inp = len({word: None for word in input_file.split()})\n",
    "unique_words_tg = len({word: None for word in target_file.split()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in English is :227 and Spanish is :355\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words in English is :{} and Spanish is :{}'.format(unique_words_inp, unique_words_tg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences_inp = eng_file.split('\\n')\n",
    "sentences_tg = spanish_file.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in English is: 137861\n",
      "Number of sentences in Spanish is: 100000\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences in English is: {}'. format(len(sentences_inp)))\n",
    "print('Number of sentences in Spanish is: {}'. format(len(sentences_tg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "words_target = [len(sentence.split()) for sentence in sentences_tg]\n",
    "words_input = [len(sentence.split()) for sentence in sentences_inp]\n",
    "avg_words_inp = (np.average(words_target))\n",
    "avg_words_tg = (np.average(words_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in English in a sentence 7.33357\n",
      "Average number of words in Spanish in a sentence 13.225277634719028\n"
     ]
    }
   ],
   "source": [
    "#number of words and average of words\n",
    "\n",
    "print('Average number of words in English in a sentence {}'.format(avg_words_inp))\n",
    "print('Average number of words in Spanish in a sentence {}'.format(avg_words_tg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in Spanish 17\n",
      "Longest sentence in English 30\n"
     ]
    }
   ],
   "source": [
    "print('Longest sentence in Spanish {}'. format(max(words_input)))\n",
    "print('Longest sentence in English {}'. format(max(words_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stats for words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.347770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words\n",
       "count  100000.000000\n",
       "mean        6.347770\n",
       "std         2.842479\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         6.000000\n",
       "75%         8.000000\n",
       "max        24.000000"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_inp = pd.DataFrame(data=words_eng, columns=['words'])\n",
    "stats_target = pd.DataFrame(data=words_spain, columns=['words'])\n",
    "stats_inp.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stats for words in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.333570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.341861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words\n",
       "count  100000.000000\n",
       "mean        7.333570\n",
       "std         3.341861\n",
       "min         1.000000\n",
       "25%         5.000000\n",
       "50%         7.000000\n",
       "75%         9.000000\n",
       "max        30.000000"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(xfile):\n",
    "        #split text file without repetition\n",
    "        vocab = set(xfile.split())\n",
    "        \n",
    "        #starting our table with CODES created on the second shell of this script, with PAD, EOS, etc\n",
    "        \n",
    "        vocab_int = copy.copy(CODES)\n",
    "        for key, value in enumerate(vocab, len(CODES)):\n",
    "            vocab_int[value] = key\n",
    "            \n",
    "        #We do the inverse process now, swapping keys and values   \n",
    "        int_to_vocab = {key:value for value, key in vocab_int.items()} \n",
    "        return vocab_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data_save(input_path, target_path, text_ids):\n",
    "        #loading files into variables\n",
    "        input_text = data_load(input_path)\n",
    "        target_text = data_load(target_path)\n",
    "        \n",
    "        #changing text files to low case\n",
    "        input_text = input_text.lower()\n",
    "        target_text = target_text.lower()\n",
    "        \n",
    "        #creating lookup table, each word will be represented by a number the two variables returned\n",
    "        #represent a way to check the 'id' of each word, similar to a dic in python (key, value)\n",
    "        input_vocab_to_int, int_input_to_vocab = create_lookup_tables(input_text)\n",
    "        target_vocab_to_int, int_target_to_vocab = create_lookup_tables(target_text)\n",
    "        \n",
    "        input_text, target_text = text_to_ids(input_text, target_text, input_vocab_to_int, target_vocab_to_int)\n",
    "        \n",
    "        \n",
    "        ##Saving Data\n",
    "        \n",
    "        pickle.dump(((input_text, target_text), \n",
    "                     (input_vocab_to_int, target_vocab_to_int), \n",
    "                     (int_input_to_vocab, int_target_to_vocab)), open('preprocess_data.p', 'wb'))\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(input_txt, target_txt, input_vocab, target_vocab):\n",
    "    \n",
    "    input_text_id = [[input_vocab[word] for word in sentence.split()] for sentence in input_txt.split('\\n')]\n",
    "    target_text_id = [[target_vocab[word] for word in sentence.split()] for \n",
    "                     sentence in target_txt.split('\\n')]\n",
    "    \n",
    "    #in case of not having EOS at the end of sentence add it\n",
    "    for i in target_text_id:\n",
    "        if i and i[-1] != target_vocab['<EOS>']:\n",
    "            i += [target_vocab['<EOS>']]\n",
    "    \n",
    "    return input_text_id, target_text_id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    return pickle.load(open('preprocess_data.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_par(par):\n",
    "    pickle.dump(par, open('par.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_par():\n",
    "    return pickle.load(open('par.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "preprocess_data_save(input_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(input_int, target_int), (source_vocab_int, target_vocab_int), _ = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Building Neural Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name='inputs')\n",
    "    target = tf.placeholder(dtype=tf.int32, shape=[None, None], name='target')\n",
    "    learn_rate = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob') #probability that each element is kept\n",
    "\n",
    "    return inputs, target, learn_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Decoding Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_input(target_data, target_vocab_int, batch_size):\n",
    "    \n",
    "    target_batches = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], target_vocab_int['<GO>']), target_batches], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encoding(rnn_input, rnn_size, num_layers, keep_prob):\n",
    "    \n",
    "    #encoder_cell = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    _, encoder_state = tf.nn.dynamic_rnn(encoder_cell, rnn_input, dtype=tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Decoder Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoder_logits(encoder_state, decoder_cell, decoder_embedded_input, seq_len, decoding_scope,\n",
    "                  output_layer, keep_prob):\n",
    "    \n",
    "    #train_decoder = tf.contrib.legacy_seq2seq.dynamic_rnn_decoder(encoder_state)\n",
    "    #contrib/seq2seq/python/ops/decoder_fn\n",
    "    \n",
    "    train_decoder = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, train_decoder, decoder_embedded_input,\n",
    "                                                             seq_len, scope=decoding_scope)\n",
    "    \n",
    "    train_logits = output_layer(train_pred)\n",
    "    train_logits = tf.nn.dropout(train_logits, keep_prob)\n",
    "    \n",
    "    return train_logits\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoder_layer_logits(encoder_state, decoder_cell, decoder_embedded_input, seq_len, decoding_scope,\n",
    "                  output_layer, keep_prob):\n",
    "    \n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_embedded_input,\n",
    "\n",
    "    initial_state=encoder_state,\n",
    "\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",)\n",
    "    \n",
    "    decoder_logits = tf.contrib.layers.linear(decoder_outputs, seq_len)\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_allowed_symbols', 'attention_decoder_fn_inference', 'attention_decoder_fn_train', 'dynamic_rnn_decoder', 'prepare_attention', 'sequence_loss', 'simple_decoder_fn_inference', 'simple_decoder_fn_train']\n"
     ]
    }
   ],
   "source": [
    "tf.__version__\n",
    "print(dir(tf.contrib.seq2seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Decoding Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_inference(encoder_state, decoder_cell, decoder_embeddings, start_seq_id, end_seq_id, \n",
    "                   maximum_len, vocab_size, decoding_scope, output_layer, prob):\n",
    "    \n",
    "    inference_decoder = tf.contrib.seq2seq.simple_decoder_fn_inference(output_layer, encoder_state,\n",
    "                                                                       decoder_embeddings, start_seq_id, end_seq_id,\n",
    "                                                                       maximum_len - 1, vocab_size)\n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, inference_decoder,\n",
    "                                                                    scope=decoding_scope)\n",
    "    \n",
    "    inference_logits = tf.nn.dropout(inference_logits, prob)\n",
    "    return inference_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(decoder_embedded_input, decoder_embeddings, encoder_state, vocab_size, \n",
    "                   seq_len, rnn_size, num_layers, target_vocab_int, prob):\n",
    "    \n",
    "    #decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    #decoder_cell_inf = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) for layer in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        output_layer = lambda x:tf.contrib.layers.fully_connected(x, vocab_size, None, scope=decoding_scope)\n",
    "        \n",
    "        train_logits = decoder_logits(encoder_state, decoder_cell, decoder_embedded_input, \n",
    "                                            seq_len, decoding_scope, output_layer, prob)\n",
    "   \n",
    "    with tf.variable_scope(\"decoding\", reuse=True) as decoding_scope_inf:\n",
    "        decoding_scope.reuse_variables()\n",
    "        inference_logits = decoding_layer_inference(encoder_state, decoder_cell, decoder_embeddings, \n",
    "                                                    target_vocab_int['<GO>'], target_vocab_int['<EOS>'], seq_len,\n",
    "                                                   vocab_size, decoding_scope, output_layer, prob)\n",
    "    return train_logits, inference_logits\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, seq_len, input_vocab_size, target_vocab_size, \n",
    "                  encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, target_vocab_int):\n",
    "    \n",
    "    \n",
    "    #Encoder Embedding\n",
    "    encoder_embedding_input = tf.contrib.layers.embed_sequence(input_data, input_vocab_size, encoder_embedding_size)\n",
    "    encoding_state = encoding(encoder_embedding_input, rnn_size, num_layers, keep_prob)\n",
    "    processed_target_data = decoding_input(target_data, target_vocab_int, batch_size)\n",
    "    \n",
    "    #Decoding Embedding\n",
    "    decoder_embedding = tf.Variable(tf.random_uniform([target_vocab_size, decoder_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embedding, processed_target_data)\n",
    "    \n",
    "    train_logits, inference_logits = decoding_layer(decoder_embed_input, decoder_embedding, encoding_state,\n",
    "                                                    target_vocab_size, seq_len, rnn_size, num_layers,\n",
    "                                                    target_vocab_int, keep_prob)\n",
    "    \n",
    "    return train_logits, inference_logits\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "\n",
    "learning_rate = 0.003\n",
    "keep_probability = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#_, (source_vocab_int, target_vocab_int),  (source_int_to_vocab, target_int_to_vocab) = preprocess()\n",
    "#print(target_int_to_vocab[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(input_int, target_int), (input_vocab_int, target_vocab_int), _ = preprocess()\n",
    "longest_seq_input = max([len(seq) for seq in input_int])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr,  keep_prob = model()\n",
    "    seq_len = tf.placeholder_with_default(longest_seq_input, None, name='seq_len')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, seq_len,\n",
    "                                                             len(input_vocab_int), len(target_vocab_int), \n",
    "                                                             encoding_embedding_size, decoding_embedding_size, \n",
    "                                                             rnn_size, num_layers, target_vocab_int)\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        #loss\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(train_logits, targets, tf.ones([input_shape[0], seq_len]))\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        #optimizer = tf.train.AdamOptimizer(lr)\n",
    "        #gradients = optimizer.compute_gradients(loss)\n",
    "        #gradients_ = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        #train_op = optimizer_apply_gradients(gradients_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    x = target.shape[1]\n",
    "    y = logits.shape[1]\n",
    "    longest_seq = max(x, y)\n",
    "    if longest_seq - x:\n",
    "        target = np.pad(target, [(0,0), (0, longest_seq - x)], 'constant')\n",
    "    if longest_seq - y:\n",
    "        logits = np.pad(logits, [(0,0), (0, longest_seq - y), (0,0)], 'constant')\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/1077 - Train Accuracy:  0.294, Validation Accuracy:  0.305, Loss:  5.901\n",
      "Epoch   0 Batch  100/1077 - Train Accuracy:  0.450, Validation Accuracy:  0.494, Loss:  3.478\n",
      "Epoch   0 Batch  200/1077 - Train Accuracy:  0.484, Validation Accuracy:  0.556, Loss:  3.170\n",
      "Epoch   0 Batch  300/1077 - Train Accuracy:  0.569, Validation Accuracy:  0.603, Loss:  3.054\n",
      "Epoch   0 Batch  400/1077 - Train Accuracy:  0.607, Validation Accuracy:  0.603, Loss:  2.913\n",
      "Epoch   0 Batch  500/1077 - Train Accuracy:  0.644, Validation Accuracy:  0.644, Loss:  2.865\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy:  0.695, Validation Accuracy:  0.673, Loss:  2.881\n",
      "Epoch   0 Batch  700/1077 - Train Accuracy:  0.701, Validation Accuracy:  0.690, Loss:  2.851\n",
      "Epoch   0 Batch  800/1077 - Train Accuracy:  0.776, Validation Accuracy:  0.732, Loss:  2.721\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy:  0.838, Validation Accuracy:  0.820, Loss:  2.818\n",
      "Epoch   0 Batch 1000/1077 - Train Accuracy:  0.833, Validation Accuracy:  0.813, Loss:  2.691\n",
      "Epoch   1 Batch    0/1077 - Train Accuracy:  0.889, Validation Accuracy:  0.850, Loss:  2.671\n",
      "Epoch   1 Batch  100/1077 - Train Accuracy:  0.873, Validation Accuracy:  0.852, Loss:  2.674\n",
      "Epoch   1 Batch  200/1077 - Train Accuracy:  0.855, Validation Accuracy:  0.899, Loss:  2.529\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy:  0.904, Validation Accuracy:  0.906, Loss:  2.592\n",
      "Epoch   1 Batch  400/1077 - Train Accuracy:  0.907, Validation Accuracy:  0.887, Loss:  2.591\n",
      "Epoch   1 Batch  500/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.881, Loss:  2.626\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy:  0.911, Validation Accuracy:  0.911, Loss:  2.634\n",
      "Epoch   1 Batch  700/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.920, Loss:  2.544\n",
      "Epoch   1 Batch  800/1077 - Train Accuracy:  0.917, Validation Accuracy:  0.925, Loss:  2.612\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy:  0.938, Validation Accuracy:  0.926, Loss:  2.650\n",
      "Epoch   1 Batch 1000/1077 - Train Accuracy:  0.910, Validation Accuracy:  0.922, Loss:  2.612\n",
      "Epoch   2 Batch    0/1077 - Train Accuracy:  0.937, Validation Accuracy:  0.934, Loss:  2.566\n",
      "Epoch   2 Batch  100/1077 - Train Accuracy:  0.918, Validation Accuracy:  0.930, Loss:  2.636\n",
      "Epoch   2 Batch  200/1077 - Train Accuracy:  0.918, Validation Accuracy:  0.939, Loss:  2.629\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.928, Loss:  2.610\n",
      "Epoch   2 Batch  400/1077 - Train Accuracy:  0.937, Validation Accuracy:  0.936, Loss:  2.672\n",
      "Epoch   2 Batch  500/1077 - Train Accuracy:  0.935, Validation Accuracy:  0.940, Loss:  2.658\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy:  0.932, Validation Accuracy:  0.930, Loss:  2.716\n",
      "Epoch   2 Batch  700/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.938, Loss:  2.649\n",
      "Epoch   2 Batch  800/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.934, Loss:  2.610\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.952, Loss:  2.626\n",
      "Epoch   2 Batch 1000/1077 - Train Accuracy:  0.927, Validation Accuracy:  0.925, Loss:  2.602\n",
      "Epoch   3 Batch    0/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.937, Loss:  2.675\n",
      "Epoch   3 Batch  100/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.944, Loss:  2.657\n",
      "Epoch   3 Batch  200/1077 - Train Accuracy:  0.931, Validation Accuracy:  0.953, Loss:  2.599\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.934, Loss:  2.629\n",
      "Epoch   3 Batch  400/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.955, Loss:  2.652\n",
      "Epoch   3 Batch  500/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.954, Loss:  2.522\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy:  0.946, Validation Accuracy:  0.931, Loss:  2.606\n",
      "Epoch   3 Batch  700/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.950, Loss:  2.651\n",
      "Epoch   3 Batch  800/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.931, Loss:  2.608\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.958, Loss:  2.714\n",
      "Epoch   3 Batch 1000/1077 - Train Accuracy:  0.947, Validation Accuracy:  0.941, Loss:  2.574\n",
      "Epoch   4 Batch    0/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.948, Loss:  2.630\n",
      "Epoch   4 Batch  100/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.941, Loss:  2.580\n",
      "Epoch   4 Batch  200/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.931, Loss:  2.581\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.950, Loss:  2.619\n",
      "Epoch   4 Batch  400/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.944, Loss:  2.552\n",
      "Epoch   4 Batch  500/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.951, Loss:  2.609\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.939, Loss:  2.738\n",
      "Epoch   4 Batch  700/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.956, Loss:  2.678\n",
      "Epoch   4 Batch  800/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.962, Loss:  2.583\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.958, Loss:  2.639\n",
      "Epoch   4 Batch 1000/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.960, Loss:  2.688\n",
      "Epoch   5 Batch    0/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.959, Loss:  2.609\n",
      "Epoch   5 Batch  100/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.948, Loss:  2.644\n",
      "Epoch   5 Batch  200/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.956, Loss:  2.588\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.955, Loss:  2.571\n",
      "Epoch   5 Batch  400/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.960, Loss:  2.608\n",
      "Epoch   5 Batch  500/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.935, Loss:  2.694\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.943, Loss:  2.620\n",
      "Epoch   5 Batch  700/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.942, Loss:  2.571\n",
      "Epoch   5 Batch  800/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.951, Loss:  2.625\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.949, Loss:  2.674\n",
      "Epoch   5 Batch 1000/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.953, Loss:  2.629\n",
      "Epoch   6 Batch    0/1077 - Train Accuracy:  0.957, Validation Accuracy:  0.965, Loss:  2.575\n",
      "Epoch   6 Batch  100/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.961, Loss:  2.607\n",
      "Epoch   6 Batch  200/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.965, Loss:  2.547\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.941, Loss:  2.652\n",
      "Epoch   6 Batch  400/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.959, Loss:  2.643\n",
      "Epoch   6 Batch  500/1077 - Train Accuracy:  0.975, Validation Accuracy:  0.966, Loss:  2.656\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.955, Loss:  2.647\n",
      "Epoch   6 Batch  700/1077 - Train Accuracy:  0.976, Validation Accuracy:  0.934, Loss:  2.655\n",
      "Epoch   6 Batch  800/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.949, Loss:  2.657\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.964, Loss:  2.649\n",
      "Epoch   6 Batch 1000/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.962, Loss:  2.615\n",
      "Epoch   7 Batch    0/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.954, Loss:  2.627\n",
      "Epoch   7 Batch  100/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.947, Loss:  2.567\n",
      "Epoch   7 Batch  200/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.958, Loss:  2.552\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.960, Loss:  2.545\n",
      "Epoch   7 Batch  400/1077 - Train Accuracy:  0.962, Validation Accuracy:  0.966, Loss:  2.651\n",
      "Epoch   7 Batch  500/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.967, Loss:  2.592\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.957, Loss:  2.665\n",
      "Epoch   7 Batch  700/1077 - Train Accuracy:  0.970, Validation Accuracy:  0.961, Loss:  2.657\n",
      "Epoch   7 Batch  800/1077 - Train Accuracy:  0.975, Validation Accuracy:  0.958, Loss:  2.668\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy:  0.962, Validation Accuracy:  0.962, Loss:  2.622\n",
      "Epoch   7 Batch 1000/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.967, Loss:  2.630\n",
      "Epoch   8 Batch    0/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.952, Loss:  2.595\n",
      "Epoch   8 Batch  100/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.965, Loss:  2.658\n",
      "Epoch   8 Batch  200/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.964, Loss:  2.697\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.955, Loss:  2.666\n",
      "Epoch   8 Batch  400/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.957, Loss:  2.622\n",
      "Epoch   8 Batch  500/1077 - Train Accuracy:  0.971, Validation Accuracy:  0.956, Loss:  2.693\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.947, Loss:  2.582\n",
      "Epoch   8 Batch  700/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.953, Loss:  2.499\n",
      "Epoch   8 Batch  800/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.965, Loss:  2.622\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.944, Loss:  2.639\n",
      "Epoch   8 Batch 1000/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.944, Loss:  2.635\n",
      "Epoch   9 Batch    0/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.955, Loss:  2.524\n",
      "Epoch   9 Batch  100/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.963, Loss:  2.658\n",
      "Epoch   9 Batch  200/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.971, Loss:  2.504\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.957, Loss:  2.547\n",
      "Epoch   9 Batch  400/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.962, Loss:  2.536\n",
      "Epoch   9 Batch  500/1077 - Train Accuracy:  0.971, Validation Accuracy:  0.959, Loss:  2.637\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.964, Loss:  2.681\n",
      "Epoch   9 Batch  700/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.974, Loss:  2.602\n",
      "Epoch   9 Batch  800/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.963, Loss:  2.515\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.954, Loss:  2.549\n",
      "Epoch   9 Batch 1000/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.952, Loss:  2.589\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "source_train = input_int[batch_size:]\n",
    "target_train = target_int[batch_size:]\n",
    "\n",
    "source_val = hp.pad_batch(input_int[:batch_size])\n",
    "target_val = hp.pad_batch(target_int[:batch_size])\n",
    "\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch, (source_batch, target_batch) in enumerate(hp.batch_data(source_train,\n",
    "                                                                               target_train, batch_size)):\n",
    "            \n",
    "            start = time.time()\n",
    "            _, loss_ = sess.run([train_op, loss], {input_data: source_batch, targets: target_batch, lr:learning_rate,\n",
    "                                                 seq_len: target_batch.shape[1], keep_prob: keep_probability})\n",
    "            batch_train_logits = sess.run(inference_logits, {input_data:source_batch, keep_prob:1.0})\n",
    "            batch_valid_logits = sess.run(inference_logits, {input_data:source_val, keep_prob:1.0})\n",
    "            \n",
    "            train = get_accuracy(target_batch, batch_train_logits)\n",
    "            valid = get_accuracy(np.array(target_val), batch_valid_logits)\n",
    "            end = time.time()\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                  .format(epoch, batch, len(input_int) // batch_size, train, valid, loss_))\n",
    "            loss_hist.append(loss_)\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plt.plot(loss_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq_to_seq(sent, vocab_int):\n",
    "    word_id = [vocab_int.get(word, vocab_int['<UNK>']) for word in sent.split()]\n",
    "    return word_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, (source_vocab_int, target_vocab_int),  (source_int_to_vocab, target_int_to_vocab) = preprocess()\n",
    "#load_path = load_par()\n",
    "load_path = load_params()\n",
    "\n",
    "#(input_int, target_int), (source_vocab_int, target_vocab_int), _ = preprocess()\n",
    "#(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<EOS>', 2: '<UNK>', 3: '<GO>', 4: 'temps', 5: 'enneigée', 6: 'juillet', 7: 'chaude', 8: \"n'aimait\", 9: 'mon', 10: 'sec', 11: 'mes', 12: 'grosse', 13: 'détendre', 14: 'préféré', 15: 'automne', 16: 'préférés', 17: 'mois', 18: 'pomme', 19: \"n'est\", 20: 'grandes', 21: 'fruits', 22: 'trop', 23: 'at', 24: \"qu'il\", 25: 'du', 26: 'brillant', 27: 'voulez', 28: 'occupé', 29: 'traduire', 30: 'chaux', 31: 'entre', 32: 'plaît', 33: 'avez', 34: 'dernière', 35: 'pêches', 36: 'prévoient', 37: 'mais', 38: 'novembre', 39: \"l'orange\", 40: 'voudrait', 41: 'allés', 42: 'son', 43: 'où', 44: 'citrons', 45: 'rouille', 46: 'france', 47: 'petit', 48: 'lui', 49: 'allions', 50: 'allez', 51: 'voulaient', 52: 'chaud', 53: 'terrain', 54: 'frais', 55: 'pleut', 56: 'grands', 57: 'de', 58: 'as-tu', 59: 'éléphants', 60: 'ne', 61: 'relaxant', 62: \"l'éléphant\", 63: 'envisage', 64: 'quand', 65: 'préférées', 66: '-il', 67: 'souris', 68: 'fait', 69: 'trouvé', 70: \"l'ours\", 71: 'vos', 72: 'requin', 73: 'portugais', 74: 'serpents', 75: 'détestez', 76: 'détend', 77: 'pamplemousse', 78: 'humide', 79: 'habituellement', 80: 'prochain', 81: 'voiture', 82: '.', 83: 'allé', 84: 'blanche', 85: 'août', 86: \"d'\", 87: 'dernier', 88: 'etats-unis', 89: 'gros', 90: 'congélation', 91: 'moins', 92: 'oiseaux', 93: 'se', 94: 'été', 95: 'cette', 96: 'envisagent', 97: 'citron', 98: \"j'aime\", 99: 'inde', 100: 'grand', 101: 'ce', 102: 'agréable', 103: 'ses', 104: 'aux', 105: 'aimez', 106: 'aimeraient', 107: 'occupée', 108: 'sont', 109: 'votre', 110: 'étaient', 111: 'le', 112: 'pense', 113: \"l'école\", 114: 'calme', 115: 'chat', 116: 'avons', 117: 'magnifique', 118: 'facile', 119: 'détestait', 120: 'des', 121: 'au', 122: 'moteur', 123: 'pluies', 124: 'cours', 125: 'gelés', 126: '-elle', 127: 'mouillée', 128: 'allée', 129: 'un', 130: 'oranges', 131: 'dans', 132: 'préférée', 133: 'raisins', 134: 'singes', 135: 'les', 136: 'visite', 137: 'tout', 138: 'maillot', 139: 'peu', 140: 'nos', 141: 'singe', 142: 'aimés', 143: 'noire', 144: 'bleu', 145: 'apprécié', 146: 'brillante', 147: 'blanc', 148: \"l'oiseau\", 149: 'redoutée', 150: 'comme', 151: \"n'aimons\", 152: 'bien', 153: '-ils', 154: \"n'êtes\", 155: 'rouillé', 156: 'verte', 157: 'california', 158: 'voulait', 159: 'douce', 160: 'ont', 161: '-ce', 162: 'il', 163: 'serpent', 164: 'pourquoi', 165: 'gel', 166: 'glaciales', 167: 'nous', 168: 'aiment', 169: 'souvent', 170: 'ils', 171: 'traduction', 172: 'enneigé', 173: 'fraises', 174: 'lapins', 175: 'éléphant', 176: 'animal', 177: 'automobile', 178: 'conduisait', 179: 'es-tu', 180: \"n'aimez\", 181: 'petits', 182: 'manguiers', 183: 'février', 184: ',', 185: \"n'aiment\", 186: 'prévoyons', 187: 'et', 188: 'en', 189: 'français', 190: 'neige', 191: 'pamplemousses', 192: 'une', 193: \"l'épicerie\", 194: 'mars', 195: 'préféré.', 196: 'mai', 197: 'favori', 198: 'noir', 199: 'aimé.', 200: 'i', 201: 'cher', 202: 'merveilleux', 203: 'bénigne', 204: 'doux', 205: 'mangue', 206: 'décembre', 207: 'vieille', 208: 'jaune', 209: 'beau', 210: 'vont', 211: '?', 212: 'était', 213: 'sèche', 214: 'états-unis', 215: 'va', 216: 'camion', 217: 'chine', 218: 'plus', 219: 'est', 220: 'vais', 221: 'eiffel', 222: 'lac', 223: 'chevaux', 224: 'football', 225: 'rouge', 226: 'pluie', 227: 'cépage', 228: 'septembre', 229: 'leur', 230: 'printemps', 231: 'favoris', 232: 'poires', 233: 'gelé', 234: 'la', 235: 'aiment-ils', 236: 'difficile', 237: 'banane', 238: 'avril', 239: 'petites', 240: 'pas', 241: 'clémentes', 242: 'rouillée', 243: 'aime', 244: 'veut', 245: 't', 246: 'bananes', 247: 'janvier', 248: 'froid', 249: 'bleue', 250: 'neigeux', 251: 'visiter', 252: 'pourrait', 253: 'aimé', 254: 'jamais', 255: 'sur', 256: 'verts', 257: 'espagnol', 258: 'pêche', 259: 'leurs', 260: 'fruit', 261: 'chats', 262: 'pendant', 263: 'à', 264: 'monde', 265: 'êtes-vous', 266: 'jersey', 267: \"n'a\", 268: 'est-ce', 269: 'pousse', 270: 'juin', 271: 'conduite', 272: 'grande', 273: 'requins', 274: 'tranquille', 275: 'redoutés', 276: 'lion', 277: 'limes', 278: 'moindres', 279: 'mangues', 280: 'redouté', 281: \"l'automobile\", 282: 'vieux', 283: \"l'\", 284: 'animaux', 285: 'vous', 286: 'cheval', 287: 'je', 288: 'intention', 289: 'petite', 290: 'vit', 291: 'raisin', 292: 'aimait', 293: 'faire', 294: 'chinois', 295: 'pluvieux', 296: 'octobre', 297: 'déteste', 298: 'aimée', 299: 'rendre', 300: 'notre', 301: 'vert', 302: 'porcelaine', 303: 'pourraient', 304: \"qu'elle\", 305: 'détestons', 306: 'aimons', 307: 'vu', 308: 'gèle', 309: 'légère', 310: 'pommes', 311: 'que', 312: \"l'automne\", 313: 'frisquet', 314: 'tour', 315: 'proches', 316: 'new', 317: 'allons', 318: 'belle', 319: 'hiver', 320: 'californie', 321: 'parfois', 322: 'oiseau', 323: 'chien', 324: 'aller', 325: 'lions', 326: 'ours', 327: 'paris', 328: 'prévois', 329: 'qui', 330: 'cet', 331: 'traduis', 332: \"n'aime\", 333: 'prévoit', 334: 'nouveau', 335: 'vers', 336: 'lapin', 337: 'elle', 338: 'conduit', 339: 'poire', 340: 'grosses', 341: 'ressort', 342: 'chiens', 343: 'fraise', 344: 'a', 345: 'mouillé', 346: \"l'animal\", 347: 'veulent', 348: '-', 349: 'volant', 350: 'amusant', 351: 'généralement', 352: 'durant', 353: 'anglais', 354: \"c'est\", 355: 'nouvelle', 356: 'pensez', 357: 'comment'}\n"
     ]
    }
   ],
   "source": [
    "print(target_int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "Words ids:       [156, 58, 44, 118, 18, 201, 196]\n",
      "English words:   ['paris', 'is', 'sometimes', 'wonderful', 'in', 'september', '.']\n",
      "\n",
      "Prediction\n",
      "Words ids:       [327, 219, 295, 188, 228, 82, 1]\n",
      "French words:   ['paris', 'est', 'pluvieux', 'en', 'septembre', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "translate = 'paris is sometimes wonderful in september .'\n",
    "#import ptf.import_graph_def.\n",
    "translate = seq_to_seq(translate, source_vocab_int)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "    \n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    translate_logits = sess.run(logits, {input_data: [translate], keep_prob: 1.0})[0]\n",
    "    \n",
    "print('Input')\n",
    "print('Words ids:       {}'. format([ids for ids in translate]))\n",
    "print('English words:   {}'. format([source_int_to_vocab[i] for i in translate]))\n",
    "\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('Words ids:       {}'. format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('French words:   {}'. format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
